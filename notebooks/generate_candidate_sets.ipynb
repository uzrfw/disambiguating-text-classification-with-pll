{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.class_type import ClassType\n",
    "from utils.category_type import CategoryType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from environment.env import getNotebookDataSourcePath\n",
    "import asyncio\n",
    "from heuristics.llama_generator import LlamaGenerator\n",
    "\n",
    "connection = sqlite3.connect(getNotebookDataSourcePath())\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load existing candidate sets\n",
    "query_llama = 'SELECT sectionID, label FROM candidate_set_llama2_1_training'\n",
    "df_llama = pd.read_sql_query(query_llama, connection)\n",
    "\n",
    "# Load wikipedia sections\n",
    "query_wikipedia_sections = 'SELECT sectionID, cleanedArticleText FROM wikipedia_sections'\n",
    "df_wikipedia_sections = pd.read_sql_query(query_wikipedia_sections, connection)\n",
    "\n",
    "# Find instances with missing candidate set\n",
    "all_section_ids = set(df_wikipedia_sections['sectionID'])\n",
    "current_section_ids = set(df_llama['sectionID'])\n",
    "missing_section_ids = sorted((all_section_ids - current_section_ids), key=lambda x: x[0])\n",
    "missing_sections = df_wikipedia_sections[df_wikipedia_sections['sectionID'].isin(missing_section_ids)]\n",
    "section_ids, section_x = np.array(np.hsplit(missing_sections, 2))\n",
    "\n",
    "async def process_section(p_sectionID, p_text):\n",
    "    connection_p = sqlite3.connect(getNotebookDataSourcePath())\n",
    "    cursor_p = connection_p.cursor()\n",
    "\n",
    "    # Load label generators\n",
    "    label_generator = LlamaGenerator(CategoryType.MEDIUM, ClassType.ORDER)\n",
    "    labels = label_generator.get_llama_labels(p_text[0])\n",
    "    labels_one = labels[:1]\n",
    "    labels_two = labels[:2]\n",
    "    labels_three = labels[:3]\n",
    "\n",
    "    # Store labels\n",
    "    processed_data_1 = [(p_sectionID[0], label) for label in labels_one]\n",
    "    labels_to_insert_1 = [(section_id, label) for section_id, label in processed_data_1]\n",
    "    cursor_p.executemany(f'INSERT INTO candidate_set_llama2_1_training (sectionID, label) VALUES (?, ?)', labels_to_insert_1)\n",
    "\n",
    "    processed_data_2 = [(p_sectionID[0], label) for label in labels_two]\n",
    "    labels_to_insert_2 = [(section_id, label) for section_id, label in processed_data_2]\n",
    "    cursor_p.executemany(f'INSERT INTO candidate_set_llama2_2_training (sectionID, label) VALUES (?, ?)', labels_to_insert_2)\n",
    "\n",
    "    processed_data_3 = [(p_sectionID[0], label) for label in labels_three]\n",
    "    labels_to_insert_3 = [(section_id, label) for section_id, label in processed_data_3]\n",
    "    cursor_p.executemany(f'INSERT INTO candidate_set_llama2_3_training (sectionID, label) VALUES (?, ?)', labels_to_insert_3)\n",
    "\n",
    "    connection_p.commit()\n",
    "    connection_p.close()\n",
    "\n",
    "# Load missing candidate sets async\n",
    "tasks = []\n",
    "for t_sectionID, t_text in zip(section_ids, section_x):\n",
    "    task = asyncio.create_task(process_section(t_sectionID, t_text))\n",
    "    tasks.append(task)\n",
    "\n",
    "await asyncio.gather(*tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
